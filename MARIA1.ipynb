{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load PDFs and Split into Chunks\n",
    "- [How to Load PDFs](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n",
    "- Each chunks contain each page of the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract ZMQ Functional Description.pdf\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"documents/\"\n",
    "pages = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            print(f\"Extract {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract text from {file_name}: {e}\")\n",
    "\n",
    "        async for page in loader.alazy_load():\n",
    "            pages.append(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "Electricity Meters IEC \n",
      "High Precision Metering \n",
      " \n",
      "Qualigrid ZMQ200, ZFQ200, ZCQ200\n",
      "E850\n",
      "Functional Description\n",
      " \n",
      " \n",
      "Date: 21.12.2011 \n",
      "File name: D000011320 E850 ZxQ Functional Description EN.docx \n",
      "  \n",
      "© Landis+Gyr D000011320 EN h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embed PDF and store in vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_wrapper = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectorstore = InMemoryVectorStore.from_documents(pages, embedding_wrapper)\n",
    "print(\"All chuncks are embedded and stored in vectorstore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_file = 'vectorstore/vectorstore.pkl'\n",
    "\n",
    "# Save vectorstore object to a file\n",
    "with open(vector_file, \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)\n",
    "print(\"Vector store saved using pickle.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load vectorstore object from a file\n",
    "with open(vector_file, \"rb\") as f:\n",
    "    vectorstore = pickle.load(f)\n",
    "print(\"Vector store loaded using pickle.\")\n",
    "\n",
    "# # Build the retriever\n",
    "# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build a retrieval from vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a retriever from a vectorstore using its .as_retriever method\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "# return a list of k page (class langchain_core.documents.base.Document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate response from query and LLM\n",
    "cmd: `` ollama pull llama3.2 `` \n",
    "\n",
    "location: C:\\Users\\<user_name>\\.ollama\\models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.invoke()`` and ``.stream()``\n",
    "- ``.invoke()`` \n",
    "    - This method runs the entire ``rag_chain`` pipeline with the provided query as input and returns the final output from ``StrOutputParser`` as a single response.\n",
    "    - In this case, ``retriever.invoke(query)`` retrieves relevant documents, ``format_docs`` formats them, the prompt is populated, and the model generates an answer, which is parsed and returned.\n",
    "- ``.stream()``\n",
    "    - This method executes the ``rag_chain`` in a streaming fashion, yielding each chunk of the generated answer as it’s produced by the model. This is useful for generating large outputs that need to be processed or displayed incrementally, enabling real-time feedback to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.7,   # Adjusts randomness\n",
    "    top_k=40,          # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\n",
    "    top_p=0.5,         # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\n",
    "    verbose=False,\n",
    "    cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ollama Documentation](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html#langchain_community.llms.ollama.Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context and sources to answer the asked question only. If you don't know the answer, say that you don't know.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Answer the following question and include all sources at the end of your respond (in format of xxx.pdf and page):\n",
    "{question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    text = ''\n",
    "    for doc in docs:\n",
    "        source = get_source_from_doc(doc)\n",
    "        content = doc.page_content\n",
    "        text += 'From '+source + content + '\\n\\n'\n",
    "    return text\n",
    "\n",
    "def format_docs_truncate(docs):\n",
    "    text = format_docs(docs)\n",
    "    return text[:5000]\n",
    "\n",
    "def get_source_from_doc(doc):\n",
    "    pp = str(doc.metadata['page']+1)\n",
    "    source = str(doc.metadata['source'])\n",
    "    start = source.find('/')\n",
    "    source = source[start+1:]\n",
    "    text = source + ', page ' + pp + '\\n'\n",
    "    return text\n",
    "\n",
    "def get_source_from_list(docs):\n",
    "    return \"\".join(get_source_from_doc(doc) for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs_truncate, \"question\": RunnablePassthrough()} # input: query, output: context (a formatted string of retrieved doc content) and question (the  query)\n",
    "    | prompt # input: context and question, output: filled prompt\n",
    "    | llm # input: filled prompt, output: string\n",
    "    | StrOutputParser() # input: string, output: processed readable response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['What is the role of the signal processor in the ZMQ metering system?',\n",
    "'What are the three meter types discussed in the manual, and how do they differ?',\n",
    "'What are the possible housing types for the ZMQ meter?',\n",
    "'What is the significance of the hardware configuration ID for the ZMQ meter?',\n",
    "'How is the network frequency calculated by the ZMQ metering system?',\n",
    "'What are the default measured quantities available in C.4 meters?',\n",
    "'Which firmware versions are used for meters with the C.2 and C.4 software configurations?',\n",
    "'How is reactive energy allocated to four quadrants in the ZMQ system?',\n",
    "'What is the purpose of the MAP120 tool mentioned in the manual?',\n",
    "'Describe the process for calculating apparent energy in ZMQ meters.',\n",
    "'How does the ZMQ system handle frequency monitoring for error detection?',\n",
    "'What standards or protocols does the ZMQ meter use for communication interfaces?',\n",
    "'Can the ZMQ meter support custom configurations for harmonic distortion analysis?',\n",
    "'What specific features make the C.7 configuration suitable for the Indian market?',\n",
    "'How does the ZMQ meter accommodate changes in energy tariffs through its configuration?',\n",
    "'What are the use cases for the additional power supply in ZMQ meters?',\n",
    "'What does the manual suggest regarding the accuracy limitations of voltage dips?',\n",
    "\"How does the manual address cybersecurity considerations in the ZMQ meter's design?\",\n",
    "'What applications are best suited for using the power quality recorder in the ZMQ meter?',\n",
    "'Does the ZMQ meter provide direct support for integration with smart grid systems?',\n",
    "'What is the latest version of the iPhone?',\n",
    "'How do solar panels convert sunlight into electricity?',\n",
    "'What are the main features of Windows 11?',\n",
    "'Who discovered the theory of relativity?',\n",
    "'How does blockchain technology work?',\n",
    "'What is the capital of Japan?',\n",
    "'Explain the process of DNA replication.',\n",
    "'What are the key features of Tesla electric cars?',\n",
    "'What are the health benefits of a Mediterranean diet?',\n",
    "'How does 5G technology differ from 4G?']\n",
    "\n",
    "def generate_response(query):\n",
    "    response = \"\"\n",
    "    start_generate_time = time.time()\n",
    "    for chunk in rag_chain.stream(query):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        response += chunk\n",
    "    finish_time = time.time()\n",
    "    return response, finish_time-start_generate_time\n",
    "\n",
    "for q in query:\n",
    "    print(f\"{q}\\n\")\n",
    "    response, generate_time = generate_response(q)\n",
    "    print(\"\\n**********\")\n",
    "\n",
    "    retrieved_docs = retriever.invoke(q)\n",
    "    sources =  get_source_from_list(retrieved_docs)\n",
    "\n",
    "    output_text = f'''\n",
    "    {q} \\n\n",
    "    {response}\\n\n",
    "    Retrieved source:\\n\n",
    "    {sources}\n",
    "    Generate time: {generate_time:.2f}\\n\n",
    "    **********\n",
    "    '''\n",
    "    with open('response.txt', 'a') as file:\n",
    "        file.write(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "1. [How-to guides](https://python.langchain.com/docs/how_to/)\n",
    "2. [How to get your RAG application to return sources](https://python.langchain.com/docs/how_to/qa_sources/)\n",
    "3. [How to stream runnables](https://python.langchain.com/docs/how_to/streaming/)\n",
    "4. [How to add message history](https://python.langchain.com/docs/how_to/message_history/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
